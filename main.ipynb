{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1484dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/moi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "tokenizer = TweetTokenizer(\n",
    "    preserve_case=False,\n",
    "    strip_handles=True,\n",
    "    reduce_len=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load datasets\n",
    "def clean(text):\n",
    "  \"\"\"\n",
    "  Cleans text by converting to lowercase, removing line breaks,\n",
    "  tabs and basic punctuation.\n",
    "  \"\"\"\n",
    "  # convert to lowercase\n",
    "  text = text.lower()\n",
    "\n",
    "  # replace line breaks, tabs and basic punctuation with spaces\n",
    "  for ch in [\"\\n\",\"\\t\",\".\", \",\", \"!\", \"?\", \":\", \";\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", '\"', \"'\"]:\n",
    "      text = text.replace(ch, \" \")\n",
    "\n",
    "  # merge multiple spaces into one\n",
    "  text = \" \".join(text.split())\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "def load_datasets():\n",
    "    \"\"\"\n",
    "    Loads Malay text dataset and cleans them.\n",
    "\n",
    "    Returns:\n",
    "        tokenized: list of tokenized sentences (list of lists of words)\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # split into a list of sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    tokenized = []\n",
    "\n",
    "    # split each sentence into a list of words\n",
    "    for s in sentences:\n",
    "        s = clean(s)\n",
    "        tokens = nltk.word_tokenize(s)\n",
    "        tokenized.append(tokens)\n",
    "\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df356eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67371/3227312817.py:30: DtypeWarning: Columns (1,3,4,5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess tweets\n",
    "def preprocess_tweets(text):\n",
    "    if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "    # Clean tweet-specific noise\n",
    "    text = p.clean(text)  # removes URLs, mentions, emojis, RT, etc.\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Keep only alphabetic tokens (supports accented chars)\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# -------- load ONE file --------\n",
    "file_path = \"tweets/extracted_data0.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# -------- clean tweets --------\n",
    "df[\"clean_text\"] = df[\"tweet_text\"].apply(preprocess_tweets)\n",
    "\n",
    "# -------- remove short tweets (<4 words) --------\n",
    "df[\"word_count\"] = df[\"clean_text\"].str.split().str.len()\n",
    "df_clean = df[df[\"word_count\"] >= 4]\n",
    "\n",
    "# -------- final list of cleaned tweets --------\n",
    "cleaned_tweets = df_clean[\"clean_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c4d543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rasanya takde mufti yang ulas so boleh ikut mufti perlis hukumnya unless ada pandangan mufti lain', 'rt amp pmr outreach sekitar kawasan taman sri petaling di parlimen p seputeh pada april kawasan yang', 'rt manga your lie in april', 'rt terkini mahkamah rayuan membenarkan sam ke ting untuk membuat rayuan sam ke ting sebelum ini dihukum penjara tahun dan', 'rt x giving away dobiesnft free mint spots rules to enter follow', 'rt gaji berpuluh ribu berbulan diam membisu rakyat berbulan bising tak ambil pusing payung kuning jentik baru tergedi', 'i don t wanna go outstation i like my room', 'rt instagram post p feel my rhythm yoon seoha', 'mesti diaorang yang terima kat ppr batu muda blok a rasa happy kan', 'rt this is it this is the video this is what you mean by being in this bangtan sonyeondan shit for life', 'rt kes penemuan mayat wanita di bukit putus dan rangka kanak lima tahun di bukit zamrud negeri sembilan dua beranak kh', 'invaded by funded by usa sympathy for ukraine save palestine first from', 'rt oopsie have you made me squirt yet if you want join me on insta babe it s free create a free account here', 'rt kakak at the petrol station counter used her mic to ask itu keta singapore ka and i pointed at the road tax on the winds', 'i don t wanna go outstation i like my room', 'rt after f months nine f months where were you all this while', 'rt the only ph presidential candidate who really wants to withdraw', 'rt ran dan mitsuya di larut malam cw implied kissing', 'done yah bismillah moga rejeki untuk berobat amp operasi batu empedu mamah makasih ka ganya aku doakan', 'invaded by funded by usa sympathy for ukraine save palestine first from']\n",
      "974799\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_tweets[:20])  # Print first 5 cleaned tweets for verification\n",
    "print(len(cleaned_tweets))  # Print first 5 cleaned tweets for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab981525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train n-gram language models\n",
    "def train_ngram_models(corpus, n=3):\n",
    "    \"\"\"\n",
    "    Train n-gram language models for Malay corpora with nltk, and download english n-gram model.\n",
    "    \"\"\"\n",
    "    training_ngrams, padded_sentences = padded_everygram_pipeline(n, corpus)\n",
    "    model = MLE(n)\n",
    "    model.fit(training_ngrams, padded_sentences)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bad95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity\n",
    "def compute_perplexity(model, text):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of all tweets with the trained n-gram models.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classify tweets\n",
    "def classify_tweets(tweets, english_model, malay_model):\n",
    "    \"\"\"\n",
    "    Classify tweets into the two types of code-mixing or discard strictly Malay tweets.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyze grammatical structure\n",
    "def analyze_grammar(tweet):\n",
    "    \"\"\"\n",
    "    Analyze the grammatical structure of a tweet to detect non-standard English patterns.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5798b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute additional statistics\n",
    "def compute_statistics(tweets):\n",
    "    \"\"\"\n",
    "    Compute additional statistics such as sentence length, CMI, Multilingual Index, etc.\n",
    "    Maybe do one cell / function per statistic instead of putting everything here.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b922cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot statistics\n",
    "def plot_statistics(statistics):\n",
    "    \"\"\"\n",
    "    Plot the computed statistics using matplotlib or seaborn.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the workflow so that the code's execution can be easily managed and stays very lisible.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
