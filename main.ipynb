{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1484dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lingl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lingl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from datasets import load_dataset\n",
    "import jsonlines\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "tokenizer = TweetTokenizer(\n",
    "    preserve_case=False,\n",
    "    strip_handles=True,\n",
    "    reduce_len=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e56f71",
   "metadata": {},
   "source": [
    "Datasets for training n-grams:  \n",
    "- \"aisyahhrazak/crawl-fiksyenshasha\" :   \n",
    "    - https://huggingface.co/datasets/aisyahhrazak/crawl-fiksyenshasha  \n",
    "    - Data scraped from https://fiksyenshasha.com/  \n",
    "    - A website for submitting (mostly horror, personal or fiction) stories.  \n",
    "    - text columns: headline, content, comment  \n",
    "\n",
    "- \"malaysia-ai/fb-malaysian-pages\" :  \n",
    "    - https://huggingface.co/datasets/malaysia-ai/fb-malaysian-pages  \n",
    "    - Data from Malaysian Facebook pages\n",
    "    - text columns: text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7176e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"aisyahhrazak/crawl-fiksyenshasha\" dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"aisyahhrazak/crawl-fiksyenshasha\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "# create a list to collect text\n",
    "all_texts_c = []\n",
    "\n",
    "for row in dataset:\n",
    "    # headline (string)\n",
    "    if row[\"headline\"]:\n",
    "        all_texts_c.append(row[\"headline\"])\n",
    "\n",
    "    # content (list of strings)\n",
    "    if row[\"content\"]:\n",
    "        all_texts_c.extend(row[\"content\"])\n",
    "\n",
    "    # comment (list of strings)\n",
    "    if row[\"comment\"]:\n",
    "        all_texts_c.extend(row[\"comment\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cae05405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected: 353160\n",
      "First 3 elements: ['KISAH SERAM JALAN SANDAKAN-KOTA KINABALU', 'Kisah ini bermula ketika saya berumur 23 tahun. Dalam umur yang masih muda saya sudah bekerja d sebuah agensi pekerjaan di sandakan. Jiwa remaja sememangnya banyak perkara yang ingin dilakukan. Saya, nana dan wan merancang untuk ke kota kinabalu awal bulan april. Seperti yang sudah di rancang. Tepat jm6 petang kami bertolak dari sandakan. Disebabkan saya masih kerja pada hari itu, kami terpaksa bertolak sebelah petang. Perjalanan dari sandakan ke kota kinabalu memakan masa 7-8jam terpulang dari kelajuan masing2. Okey pada jam7 malam kami singgah mengisi minyak di check point dan membeli makanan ringan utk mengisi perut dalam perjalanan.', 'Semasa kami melalui jalan telupid ranau hati saya merasa tidak sedap. Tapi sebagai pemandu saya kuatkan radio sekali sekala ikut menyanyi. Dalam hati syukur ada bas di belakang. Semasa melalui jlan yang agak gelap (jalan telupid ranau ni memang hutan tiada lampu jalan) bas tersebut sekali sekala memberi isyarat menggunakan high light dalam hati saya bas ni mahu memotong ke apa. Tiba2 wan yang duduk belakang bersuara ” tutup cermin wei” aku menurut saja. Budak nana ni siap tanya ,kenapa?. “Orang suruh tutup, tutup je lah, bodoh” marah wan. Dalam masa yang sama kereta yang sy pandu agak berat. Bas di belakang juga masih memberi isyarat. Dalam hati sy, tolonglah jangan ada benda yang tak diingin berlaku, perjalanan masih jauh ni. Aku pusingkan lagu rohani, syukur kereta yang saya pandu kembali ringan.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Collected:\", len(all_texts_c))\n",
    "print(\"First 3 elements:\", all_texts_c[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa161291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"malaysia-ai/fb-malaysian-pages\" dataset\n",
    "# Dataset formatting issue, need to manually read JSONL (inconsistent data types, JSON objects or strings)\n",
    "\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"malaysia-ai/fb-malaysian-pages\",\n",
    "    filename=\"dedup.jsonl\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "all_texts_f = []\n",
    "\n",
    "with jsonlines.open(file_path) as reader:\n",
    "    for obj in reader:\n",
    "        # normal JSON object\n",
    "        if isinstance(obj, dict):\n",
    "            text = obj.get(\"text\")\n",
    "\n",
    "        # string\n",
    "        elif isinstance(obj, str):\n",
    "            text = obj\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if isinstance(text, str) and text.strip():\n",
    "            all_texts_f.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce4706bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected: 193363\n",
      "First 3 elements: ['Adeyyy pooodahhh puiii', 'Bosan la ceramah dia.. dulu sonok gak', 'Memalukan betul.. PM malaysia.\\nDunia sedang memerhatikan Malaysia dipimpin oleh orang yang tercemar dgn kes2 mahkamah... \\n\\nMalaysia PM Picks Graft-Tainted Leader as One of His Deputies https://\\nwww.bloomberg.co\\nm/news/\\narticles/\\n2022-12-02/\\nmalaysia-pm-pick\\ns-graft-tainted\\n-leader-as-one-\\nof-his-deputies']\n"
     ]
    }
   ],
   "source": [
    "print(\"Collected:\", len(all_texts_f))\n",
    "print(\"First 3 elements:\", all_texts_f[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a0b3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process datasets\n",
    "def process_datasets(text):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes text.\n",
    "\n",
    "    Returns:\n",
    "        tokenized: list of tokenized sentences (list of lists of words)\n",
    "    \"\"\"\n",
    "    # get rid of URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "\n",
    "    # split into a list of sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    tokenized = []\n",
    "\n",
    "    # split each sentence into a list of words\n",
    "    for s in sentences:\n",
    "\n",
    "        # clean: to lowercase, only \n",
    "        s = s.lower()\n",
    "        s = re.sub(r\"[^a-zàâéèêëîïôûùüçñ\\s]\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "        tokens = nltk.word_tokenize(s)\n",
    "        tokenized.append(tokens)\n",
    "\n",
    "    return tokenized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede15c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_list(texts):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes a list of texts.\n",
    "    \"\"\"\n",
    "    all_tokenized = []\n",
    "\n",
    "    for text in texts:\n",
    "        tokenized = process_datasets(text)\n",
    "        all_tokenized.extend(tokenized)\n",
    "\n",
    "    return all_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9dad385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokenized_c = tokenize_list(all_texts_c)\n",
    "all_tokenized_f = tokenize_list(all_texts_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2d6ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized stories: [['kisah', 'seram', 'jalan', 'sandakan', 'kota', 'kinabalu'], ['kisah', 'ini', 'bermula', 'ketika', 'saya', 'berumur', 'tahun'], ['dalam', 'umur', 'yang', 'masih', 'muda', 'saya', 'sudah', 'bekerja', 'd', 'sebuah', 'agensi', 'pekerjaan', 'di', 'sandakan']]\n",
      "Number of sentences: 1241358\n",
      "Tokenized Facebook posts: [['adeyyy', 'pooodahhh', 'puiii'], ['bosan', 'la', 'ceramah', 'dia', 'dulu', 'sonok', 'gak'], ['memalukan', 'betul', 'pm', 'malaysia']]\n",
      "Number of sentences: 241103\n",
      "All: 1482461\n"
     ]
    }
   ],
   "source": [
    "all_tokenized = all_tokenized_c + all_tokenized_f\n",
    "\n",
    "print(\"Tokenized stories:\", all_tokenized_c[:3])\n",
    "print(\"Number of sentences:\", len(all_tokenized_c))\n",
    "\n",
    "print(\"Tokenized Facebook posts:\", all_tokenized_f[:3])\n",
    "print(\"Number of sentences:\", len(all_tokenized_f))\n",
    "\n",
    "print(\"All:\", len(all_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879423d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df356eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67371/3227312817.py:30: DtypeWarning: Columns (1,3,4,5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess tweets\n",
    "def preprocess_tweets(text):\n",
    "    if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "    # Clean tweet-specific noise\n",
    "    text = p.clean(text)  # removes URLs, mentions, emojis, RT, etc.\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Keep only alphabetic tokens (supports accented chars)\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# -------- load ONE file --------\n",
    "file_path = \"tweets/extracted_data0.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# -------- clean tweets --------\n",
    "df[\"clean_text\"] = df[\"tweet_text\"].apply(preprocess_tweets)\n",
    "\n",
    "# -------- remove short tweets (<4 words) --------\n",
    "df[\"word_count\"] = df[\"clean_text\"].str.split().str.len()\n",
    "df_clean = df[df[\"word_count\"] >= 4]\n",
    "\n",
    "# -------- final list of cleaned tweets --------\n",
    "cleaned_tweets = df_clean[\"clean_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c4d543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rasanya takde mufti yang ulas so boleh ikut mufti perlis hukumnya unless ada pandangan mufti lain', 'rt amp pmr outreach sekitar kawasan taman sri petaling di parlimen p seputeh pada april kawasan yang', 'rt manga your lie in april', 'rt terkini mahkamah rayuan membenarkan sam ke ting untuk membuat rayuan sam ke ting sebelum ini dihukum penjara tahun dan', 'rt x giving away dobiesnft free mint spots rules to enter follow', 'rt gaji berpuluh ribu berbulan diam membisu rakyat berbulan bising tak ambil pusing payung kuning jentik baru tergedi', 'i don t wanna go outstation i like my room', 'rt instagram post p feel my rhythm yoon seoha', 'mesti diaorang yang terima kat ppr batu muda blok a rasa happy kan', 'rt this is it this is the video this is what you mean by being in this bangtan sonyeondan shit for life', 'rt kes penemuan mayat wanita di bukit putus dan rangka kanak lima tahun di bukit zamrud negeri sembilan dua beranak kh', 'invaded by funded by usa sympathy for ukraine save palestine first from', 'rt oopsie have you made me squirt yet if you want join me on insta babe it s free create a free account here', 'rt kakak at the petrol station counter used her mic to ask itu keta singapore ka and i pointed at the road tax on the winds', 'i don t wanna go outstation i like my room', 'rt after f months nine f months where were you all this while', 'rt the only ph presidential candidate who really wants to withdraw', 'rt ran dan mitsuya di larut malam cw implied kissing', 'done yah bismillah moga rejeki untuk berobat amp operasi batu empedu mamah makasih ka ganya aku doakan', 'invaded by funded by usa sympathy for ukraine save palestine first from']\n",
      "974799\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_tweets[:20])  # Print first 5 cleaned tweets for verification\n",
    "print(len(cleaned_tweets))  # Print first 5 cleaned tweets for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab981525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train n-gram language models\n",
    "def train_ngram_models(corpus, n=3):\n",
    "    \"\"\"\n",
    "    Train n-gram language models for Malay corpora with nltk, and download english n-gram model.\n",
    "    \"\"\"\n",
    "    training_ngrams, padded_sentences = padded_everygram_pipeline(n, corpus)\n",
    "    model = MLE(n)\n",
    "    model.fit(training_ngrams, padded_sentences)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e90dc3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ms = train_ngram_models(all_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bad95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity\n",
    "def compute_perplexity(model, text):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of all tweets with the trained n-gram models.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classify tweets\n",
    "def classify_tweets(tweets, english_model, malay_model):\n",
    "    \"\"\"\n",
    "    Classify tweets into the two types of code-mixing or discard strictly Malay tweets.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyze grammatical structure\n",
    "def analyze_grammar(tweet):\n",
    "    \"\"\"\n",
    "    Analyze the grammatical structure of a tweet to detect non-standard English patterns.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5798b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute additional statistics\n",
    "def compute_statistics(tweets):\n",
    "    \"\"\"\n",
    "    Compute additional statistics such as sentence length, CMI, Multilingual Index, etc.\n",
    "    Maybe do one cell / function per statistic instead of putting everything here.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b922cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot statistics\n",
    "def plot_statistics(statistics):\n",
    "    \"\"\"\n",
    "    Plot the computed statistics using matplotlib or seaborn.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the workflow so that the code's execution can be easily managed and stays very lisible.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
